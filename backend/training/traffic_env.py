"""
Traffic Signal Control Environment - Research Grade
====================================================
State-of-the-art RL environment for adaptive traffic signal control.

Features:
âœ“ Realistic traffic dynamics (Poisson arrivals, queuing theory)
âœ“ Multi-objective reward function (efficiency + equity + stability)
âœ“ Configurable signal phasing (4-phase standard intersection)
âœ“ Safety constraints (min green, yellow clearance)
âœ“ Rich metrics for analysis (waiting time, throughput, fairness)
âœ“ Gym-compatible interface (OpenAI Gym standard)

Based on:
- Webster (1958) traffic signal timing theory
- Roess et al. (2004) Traffic Engineering
- Modern RL research (PPO, SAC compatible)

Author: Traffic Control RL System
Version: 3.0 - Production Ready
"""

import numpy as np
from enum import IntEnum
from typing import Tuple, Dict, Optional, List
import random
from collections import deque


# ============================================
# SIGNAL PHASE DEFINITIONS
# ============================================

class Phase(IntEnum):
    """
    Standard 4-phase signal structure.
    Industry-standard NEMA phasing adapted for RL.
    """
    NS_THROUGH = 0    # North-South through traffic + right turns
    NS_LEFT = 1       # North-South protected left turns
    EW_THROUGH = 2    # East-West through traffic + right turns
    EW_LEFT = 3       # East-West protected left turns


# ============================================
# SYNTHETIC TRAFFIC GENERATORS (TRAINING)
# ============================================

class TrafficPatternGenerator:
    """
    Generates diverse synthetic traffic patterns for robust RL training.
    
    Design Philosophy:
    - Training data should be MORE diverse than real-world test scenarios
    - Includes edge cases (starvation, saturation, asymmetry)
    - No city-specific bias (ensures generalization)
    """
    
    @staticmethod
    def uniform_random(min_rate: float = 0.1, max_rate: float = 2.0) -> Dict[str, float]:
        """Uniform random rates (baseline diversity)."""
        return {d: np.random.uniform(min_rate, max_rate) 
                for d in ['north', 'south', 'east', 'west']}
    
    @staticmethod
    def rush_hour(dominant_corridor: str = None) -> Dict[str, float]:
        """
        Simulates rush hour with asymmetric flow.
        
        Args:
            dominant_corridor: 'NS' or 'EW' (random if None)
        """
        if dominant_corridor is None:
            dominant_corridor = random.choice(['NS', 'EW'])
        
        if dominant_corridor == 'NS':
            # North-South congested
            return {
                'north': np.random.uniform(2.0, 3.5),
                'south': np.random.uniform(2.0, 3.5),
                'east': np.random.uniform(0.3, 0.8),
                'west': np.random.uniform(0.3, 0.8),
            }
        else:
            # East-West congested
            return {
                'north': np.random.uniform(0.3, 0.8),
                'south': np.random.uniform(0.3, 0.8),
                'east': np.random.uniform(2.0, 3.5),
                'west': np.random.uniform(2.0, 3.5),
            }
    
    @staticmethod
    def directional_peak(peak_direction: str = None) -> Dict[str, float]:
        """
        Single direction dominates (e.g., stadium exodus, school dismissal).
        
        Tests agent's ability to handle extreme imbalance.
        """
        if peak_direction is None:
            peak_direction = random.choice(['north', 'south', 'east', 'west'])
        
        rates = {d: np.random.uniform(0.1, 0.4) for d in ['north', 'south', 'east', 'west']}
        rates[peak_direction] = np.random.uniform(3.0, 5.0)  # Very heavy
        return rates
    
    @staticmethod
    def balanced_flow() -> Dict[str, float]:
        """Symmetric traffic (ideal scenario)."""
        ns_rate = np.random.uniform(0.5, 2.0)
        ew_rate = np.random.uniform(0.5, 2.0)
        return {
            'north': ns_rate,
            'south': ns_rate,
            'east': ew_rate,
            'west': ew_rate,
        }
    
    @staticmethod
    def low_traffic() -> Dict[str, float]:
        """Off-peak / night-time (tests efficiency in low load)."""
        return {d: np.random.uniform(0.05, 0.3) for d in ['north', 'south', 'east', 'west']}
    
    @staticmethod
    def gridlock_scenario() -> Dict[str, float]:
        """All directions saturated (stress test)."""
        return {d: np.random.uniform(2.5, 4.0) for d in ['north', 'south', 'east', 'west']}
    
    @staticmethod
    def oscillating_demand() -> Dict[str, float]:
        """
        Time-varying demand (simulates real traffic fluctuations).
        Used for longer episodes with non-stationary arrivals.
        """
        # This would be called every N steps to update rates
        return TrafficPatternGenerator.uniform_random(0.5, 2.5)
    
    @classmethod
    def get_random_pattern(cls) -> Dict[str, float]:
        """Sample a random pattern type."""
        patterns = [
            cls.uniform_random,
            cls.rush_hour,
            cls.directional_peak,
            cls.balanced_flow,
            cls.low_traffic,
            cls.gridlock_scenario,
        ]
        return random.choice(patterns)()


# ============================================
# MAIN TRAFFIC ENVIRONMENT
# ============================================

class TrafficEnv:
    """
    Research-grade traffic signal control environment.
    
    STATE SPACE (11 dimensions):
    - Queue lengths (4): Normalized [0, 1]
    - Current phase (4): One-hot encoding
    - Phase timer (1): Normalized [0, 1]
    - Queue imbalance (1): Std dev of queues
    - Throughput rate (1): Recent vehicles served
    
    ACTION SPACE (5 discrete actions):
    - 0: Extend current green phase
    - 1-4: Switch to specific phase (0-3)
    
    REWARD FUNCTION (multi-objective):
    - Primary: Minimize total waiting time
    - Secondary: Ensure fairness (penalize starvation)
    - Tertiary: Encourage stability (penalize thrashing)
    
    CONSTRAINTS:
    - Minimum green time (safety)
    - Maximum green time (fairness)
    - Yellow clearance interval (mandatory)
    """
    
    def __init__(
        self,
        # Episode parameters
        max_steps: int = 3600,              # 1 hour = 3600 seconds
        timestep_duration: float = 1.0,     # Each step = 1 second (DO NOT CHANGE)
        
        # Signal timing constraints
        min_green_time: int = 8,            # Min green (safety)
        max_green_time: int = 60,           # Max green (fairness)
        yellow_time: int = 3,               # Yellow clearance (safety)
        all_red_time: int = 2,              # All-red clearance (safety)
        
        # Traffic parameters
        saturation_flow_rate: float = 0.53, # veh/sec/lane (1900 veh/hr/lane)
        lanes_per_direction: int = 2,       # Lanes serving each direction
        max_queue_length: int = 50,         # Max vehicles in queue (per direction)
        
        # Arrival rates (can be overridden in reset)
        arrival_rates: Optional[Dict[str, float]] = None,
        
        # Advanced options
        enable_yellow_phase: bool = True,   # Force yellow transitions
        stochastic_departures: bool = True, # Add randomness to departures
        track_waiting_time: bool = True,    # Track individual vehicle wait times
    ):
        """
        Initialize environment with comprehensive configuration.
        
        TIME SEMANTICS (CRITICAL):
        - timestep_duration = 1.0 second (fixed)
        - arrival_rates MUST be in vehicles/second
        - All timing parameters in seconds
        """
        # Store configuration
        self.max_steps = max_steps
        self.timestep_duration = timestep_duration
        self.min_green_time = min_green_time
        self.max_green_time = max_green_time
        self.yellow_time = yellow_time
        self.all_red_time = all_red_time
        self.saturation_flow_rate = saturation_flow_rate
        self.lanes_per_direction = lanes_per_direction
        self.max_queue_length = max_queue_length
        self.enable_yellow_phase = enable_yellow_phase
        self.stochastic_departures = stochastic_departures
        self.track_waiting_time = track_waiting_time
        
        # Direction labels
        self.directions = ['north', 'south', 'east', 'west']
        
        # Phase-to-direction mapping (which directions get green)
        self.phase_to_directions = {
            Phase.NS_THROUGH: ['north', 'south'],
            Phase.NS_LEFT: ['north', 'south'],      # Simplified: left + through
            Phase.EW_THROUGH: ['east', 'west'],
            Phase.EW_LEFT: ['east', 'west'],
        }
        
        # Default arrival rates (will be overridden in reset)
        self.arrival_rates = arrival_rates
        
        # State variables (initialized in reset)
        self.queues = None
        self.current_phase = None
        self.phase_timer = None
        self.step_count = None
        self.total_waiting_time = None
        self.total_throughput = None
        self.in_yellow_phase = None
        self.yellow_timer = None
        self.recent_throughput = None  # For state observation
        
        # Waiting time tracking (optional)
        if self.track_waiting_time:
            self.vehicle_wait_times = {d: deque() for d in self.directions}
        
        # Action and state space dimensions
        self.n_actions = 5  # extend + 4 phase switches
        self.state_dim = 11  # 4 queues + 4 phase + timer + imbalance + throughput
    
    def reset(self, arrival_rates: Optional[Dict[str, float]] = None) -> np.ndarray:
        """
        Reset environment to initial state.
        
        Args:
            arrival_rates: Dict of arrival rates (veh/sec) per direction.
                          If None, uses synthetic pattern.
        
        Returns:
            Initial state vector
        """
        # Initialize queues (small random initial state)
        self.queues = {d: np.random.randint(0, 5) for d in self.directions}
        
        # Random initial phase (avoid bias)
        self.current_phase = Phase(np.random.randint(0, 4))
        self.phase_timer = np.random.randint(self.min_green_time, self.max_green_time // 2)
        
        # Yellow phase tracking
        self.in_yellow_phase = False
        self.yellow_timer = 0
        
        # Reset counters
        self.step_count = 0
        self.total_waiting_time = 0.0
        self.total_throughput = 0
        self.recent_throughput = deque(maxlen=60)  # Last 60 seconds
        
        # Reset wait time tracking
        if self.track_waiting_time:
            self.vehicle_wait_times = {d: deque() for d in self.directions}
        
        # Set arrival rates
        if arrival_rates is not None:
            self.arrival_rates = arrival_rates
        elif self.arrival_rates is None:
            # Generate random synthetic pattern
            self.arrival_rates = TrafficPatternGenerator.get_random_pattern()
        
        return self._get_state()
    
    def _get_state(self) -> np.ndarray:
        """
        Construct state observation vector.
        
        Returns:
            11D state: [queues(4), phase_onehot(4), timer(1), imbalance(1), throughput(1)]
        """
        # 1. Normalized queue lengths
        queue_state = np.array([
            self.queues[d] / self.max_queue_length for d in self.directions
        ])
        
        # 2. One-hot phase encoding
        phase_onehot = np.zeros(4)
        if not self.in_yellow_phase:
            phase_onehot[int(self.current_phase)] = 1.0
        # Note: During yellow, all zeros (transition state)
        
        # 3. Normalized phase timer
        timer_norm = self.phase_timer / self.max_green_time
        
        # 4. Queue imbalance (fairness metric)
        queue_values = list(self.queues.values())
        queue_imbalance = np.std(queue_values) / (self.max_queue_length + 1e-6)
        
        # 5. Recent throughput (efficiency metric)
        avg_throughput = np.mean(self.recent_throughput) if len(self.recent_throughput) > 0 else 0.0
        throughput_norm = avg_throughput / (self.saturation_flow_rate * self.lanes_per_direction)
        
        return np.concatenate([
            queue_state,          # [4]
            phase_onehot,         # [4]
            [timer_norm],         # [1]
            [queue_imbalance],    # [1]
            [throughput_norm],    # [1]
        ])
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """
        Execute one timestep (1 second).
        
        Args:
            action: 0=extend, 1-4=switch to phase (action-1)
        
        Returns:
            (next_state, reward, done, info)
        """
        self.step_count += 1
        phase_switched = False
        action_penalty = 0.0
        
        # ============================================
        # YELLOW PHASE HANDLING (Safety-critical)
        # ============================================
        if self.in_yellow_phase:
            # During yellow: count down, ignore actions
            self.yellow_timer -= 1
            if self.yellow_timer <= 0:
                # Yellow complete: transition to new phase
                self.in_yellow_phase = False
                self.phase_timer = self.min_green_time
        
        # ============================================
        # ACTION PROCESSING (Only if not in yellow)
        # ============================================
        elif not self.in_yellow_phase:
            # Check if phase timer expired (forced switch for fairness)
            if self.phase_timer <= 0:
                # Force round-robin switch
                next_phase = Phase((self.current_phase + 1) % 4)
                self._initiate_phase_change(next_phase)
                phase_switched = True
                action_penalty = 0.0  # No penalty for forced switch
            
            else:
                # Process agent action
                if action == 0:
                    # Extend current green (if under max)
                    if self.phase_timer < self.max_green_time:
                        self.phase_timer += 1
                    # No penalty for extending
                
                else:
                    # Request phase switch
                    requested_phase = Phase(action - 1)
                    
                    if requested_phase != self.current_phase:
                        # Only switch if minimum green satisfied
                        if self.phase_timer >= (self.max_green_time - self.min_green_time):
                            # Initiate yellow phase
                            self._initiate_phase_change(requested_phase)
                            phase_switched = True
                            action_penalty = 0.15  # Small penalty for switching
                        else:
                            # Too early to switch: ignore action, apply penalty
                            action_penalty = 0.05
        
        # ============================================
        # TRAFFIC DYNAMICS
        # ============================================
        
        # 1. Vehicle arrivals (Poisson process)
        for direction in self.directions:
            lambda_rate = self.arrival_rates[direction]
            arrivals = np.random.poisson(lambda_rate * self.timestep_duration)
            
            # Add to queue (with capacity limit)
            new_queue = min(
                self.queues[direction] + arrivals,
                self.max_queue_length
            )
            self.queues[direction] = new_queue
            
            # Track waiting time (if enabled)
            if self.track_waiting_time:
                for _ in range(arrivals):
                    self.vehicle_wait_times[direction].append(0)  # New vehicle wait = 0
        
        # 2. Vehicle departures (only if green)
        step_throughput = 0
        if not self.in_yellow_phase:  # No departures during yellow
            green_directions = self.phase_to_directions[self.current_phase]
            
            for direction in green_directions:
                # Calculate departure capacity
                base_departures = self.saturation_flow_rate * self.lanes_per_direction * self.timestep_duration
                
                # Add stochasticity (realistic variation)
                if self.stochastic_departures:
                    departures = int(np.random.normal(base_departures, base_departures * 0.1))
                    departures = max(0, departures)  # No negative
                else:
                    departures = int(base_departures)
                
                # Apply to queue
                actual_departures = min(departures, self.queues[direction])
                self.queues[direction] -= actual_departures
                self.total_throughput += actual_departures
                step_throughput += actual_departures
                
                # Remove from wait time tracking
                if self.track_waiting_time:
                    for _ in range(actual_departures):
                        if len(self.vehicle_wait_times[direction]) > 0:
                            self.vehicle_wait_times[direction].popleft()
        
        # Track recent throughput
        self.recent_throughput.append(step_throughput)
        
        # 3. Increment waiting times for all queued vehicles
        if self.track_waiting_time:
            for direction in self.directions:
                for i in range(len(self.vehicle_wait_times[direction])):
                    self.vehicle_wait_times[direction][i] += self.timestep_duration
        
        # 4. Decrement phase timer (if green)
        if not self.in_yellow_phase:
            self.phase_timer = max(0, self.phase_timer - 1)
        
        # ============================================
        # REWARD CALCULATION (Multi-objective)
        # ============================================
        reward = self._calculate_reward(action_penalty, phase_switched)
        
        # ============================================
        # TERMINATION CHECK
        # ============================================
        done = self.step_count >= self.max_steps
        
        # ============================================
        # INFO DICTIONARY (Metrics & Debugging)
        # ============================================
        total_queue = sum(self.queues.values())
        self.total_waiting_time += total_queue  # Cumulative queue-seconds
        
        info = {
            # Queue state
            'queues': self.queues.copy(),
            'total_queue': total_queue,
            'avg_queue': total_queue / 4,
            'max_queue': max(self.queues.values()),
            'min_queue': min(self.queues.values()),
            
            # Phase info
            'phase': int(self.current_phase),
            'phase_name': self.current_phase.name,
            'in_yellow': self.in_yellow_phase,
            'phase_timer': self.phase_timer,
            
            # Performance metrics
            'total_waiting_time': self.total_waiting_time,
            'total_throughput': self.total_throughput,
            'step_throughput': step_throughput,
            'avg_waiting_time': self.total_waiting_time / max(1, self.total_throughput),
            
            # Reward components (for analysis)
            'reward': reward,
            'phase_switched': phase_switched,
            
            # Arrival rates (for logging)
            'arrival_rates': self.arrival_rates.copy(),
        }
        
        return self._get_state(), reward, done, info
    
    def _initiate_phase_change(self, new_phase: Phase):
        """Handle phase transition with yellow clearance."""
        if self.enable_yellow_phase:
            self.in_yellow_phase = True
            self.yellow_timer = self.yellow_time
            self.current_phase = new_phase  # Store target phase
        else:
            # Immediate switch (no yellow)
            self.current_phase = new_phase
            self.phase_timer = self.min_green_time
    
    def _calculate_reward(self, action_penalty: float, phase_switched: bool) -> float:
        """
        Multi-objective reward function.
        
        Components:
        1. Queue minimization (primary objective)
        2. Fairness (prevent starvation)
        3. Stability (discourage thrashing)
        4. Throughput bonus (encourage efficiency)
        
        Returns:
            Scalar reward (typically negative, closer to 0 is better)
        """
        # 1. PRIMARY: Total queue penalty
        total_queue = sum(self.queues.values())
        queue_penalty = -total_queue / (self.max_queue_length * 4)  # Normalized
        
        # 2. FAIRNESS: Penalize queue imbalance
        queue_values = list(self.queues.values())
        queue_std = np.std(queue_values)
        fairness_penalty = -0.2 * (queue_std / (self.max_queue_length + 1e-6))
        
        # 3. STARVATION: Heavy penalty if any queue is very long
        max_queue = max(queue_values)
        if max_queue > 0.8 * self.max_queue_length:
            starvation_penalty = -0.5
        else:
            starvation_penalty = 0.0
        
        # 4. THROUGHPUT: Reward for serving vehicles
        recent_avg = np.mean(self.recent_throughput) if len(self.recent_throughput) > 0 else 0
        throughput_bonus = 0.1 * (recent_avg / (self.saturation_flow_rate * 2))  # Normalized
        
        # 5. ACTION PENALTY: Discourage excessive switching
        switching_penalty = -action_penalty if phase_switched else 0.0
        
        # Total reward
        reward = (
            queue_penalty +
            fairness_penalty +
            starvation_penalty +
            throughput_bonus +
            switching_penalty
        )
        
        return reward
    
    def get_metrics(self) -> Dict:
        """
        Get comprehensive performance metrics.
        
        Returns:
            Dictionary of KPIs for evaluation
        """
        total_queue = sum(self.queues.values())
        
        return {
            # Efficiency
            'avg_queue_length': self.total_waiting_time / max(1, self.step_count),
            'total_throughput': self.total_throughput,
            'throughput_per_step': self.total_throughput / max(1, self.step_count),
            
            # Fairness
            'final_queues': self.queues.copy(),
            'queue_imbalance': np.std(list(self.queues.values())),
            'max_queue_reached': max(self.queues.values()),
            
            # Waiting time
            'total_delay_seconds': self.total_waiting_time,
            'avg_vehicle_delay': self.total_waiting_time / max(1, self.total_throughput),
            
            # Episode info
            'steps': self.step_count,
            'duration_hours': self.step_count / 3600,
        }


# ============================================
# TESTING & DEMONSTRATION
# ============================================

if __name__ == "__main__":
    print("=" * 80)
    print("ðŸš¦ TRAFFIC SIGNAL CONTROL ENVIRONMENT - RESEARCH GRADE")
    print("=" * 80)
    
    # Create environment with realistic parameters
    env = TrafficEnv(
        max_steps=1800,              # 30-minute episodes
        saturation_flow_rate=0.53,   # 1900 veh/hr/lane
        lanes_per_direction=2,
    )
    
    # Test on different traffic scenarios
    test_scenarios = [
        ("Low Traffic (Night)", TrafficPatternGenerator.low_traffic()),
        ("Balanced Flow", TrafficPatternGenerator.balanced_flow()),
        ("Rush Hour (NS)", TrafficPatternGenerator.rush_hour('NS')),
        ("Gridlock", TrafficPatternGenerator.gridlock_scenario()),
    ]
    
    for scenario_name, arrival_rates in test_scenarios:
        print(f"\n{'='*80}")
        print(f"ðŸ“Š SCENARIO: {scenario_name}")
        print(f"{'='*80}")
        print(f"Arrival Rates:")
        for direction, rate in arrival_rates.items():
            print(f"  {direction.capitalize():5s}: {rate:.2f} veh/sec")
        
        # Run episode with random policy
        state = env.reset(arrival_rates=arrival_rates)
        total_reward = 0.0
        
        for step in range(1800):
            action = np.random.randint(0, 5)  # Random baseline
            state, reward, done, info = env.step(action)
            total_reward += reward
            
            if done:
                break
        
        # Print metrics
        metrics = env.get_metrics()
        print(f"\nðŸ“ˆ Performance Metrics:")
        print(f"  Total Throughput: {metrics['total_throughput']} vehicles")
        print(f"  Avg Queue Length: {metrics['avg_queue_length']:.2f} vehicles")
        print(f"  Avg Vehicle Delay: {metrics['avg_vehicle_delay']:.2f} seconds")
        print(f"  Queue Imbalance (Ïƒ): {metrics['queue_imbalance']:.2f}")
        print(f"  Total Reward: {total_reward:.2f}")
        print(f"  Final Queues: {metrics['final_queues']}")
    
    print(f"\n{'='*80}")
    print("âœ… Environment validated! Ready for RL training.")
    print("State dim:", env.state_dim)
    print("Action dim:", env.n_actions)
    print("=" * 80)